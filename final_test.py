# -*- coding: utf-8 -*-
"""Final_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bzlHI5W-QIArRE2QjFzGvP_cqzv4Uke3
"""

!pip install streamlit

!pip install kaggle

!mkdir -p ~/.kaggle/

!gdown --id 1FHjv-5CwDoHOP7cw3HIniWIblXhQJFKr

!cp /content/kaggle.json ~/.kaggle

!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d mohamedhanyyy/chest-ctscan-images

!ls

!unzip chest-ctscan-images.zip

import numpy as np
  import pandas as pd
  import cv2
  import keras
  from numpy import random
  import tensorflow as tf
  import tensorflow_datasets as tfds
  from matplotlib import pyplot as plt
  from keras.models import Sequential, Model
  from keras.layers import Dense, Dropout, Flatten, Conv2D
  from keras.layers import MaxPooling2D, BatchNormalization
  from keras.optimizers import SGD
  #from keras.utils import np_utils
  from keras.applications import MobileNet, VGG16, ResNet50,DenseNet201,InceptionV3,EfficientNetB7
  from sklearn.metrics import roc_curve, auc
  from tensorflow.keras.applications.resnet50 import preprocess_input
  from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
  from sklearn.metrics import precision_score, recall_score, accuracy_score
  from sklearn.metrics import classification_report ,confusion_matrix
  from keras.preprocessing.image import ImageDataGenerator
  from sklearn.model_selection import train_test_split
  from tabulate import tabulate
  import seaborn as sns
  import os
  from keras.layers import Input, Dense, MaxPool2D, Conv2D, BatchNormalization, Flatten,Dropout, MaxPooling2D,UpSampling2D
  from keras.models import Sequential
  from keras.layers import Dense, Dropout, Activation, Flatten
  from tensorflow.keras.models import Model
  from tensorflow.keras import layers, losses
  from keras.optimizers import Adam
  from keras import models
  from PIL import Image
  from tensorflow.keras.preprocessing import image

train_folder = "/content/Data/train"
valid_folder = "/content/Data/valid"
test_folder = "/content/Data/test"

"""#### Обработка данных"""

print("\n\n\t\tTraining Set")
print("\t  ========================\n")
train_c = []
for folder in os.listdir(train_folder):
    train_c.append(folder)
    print("\nTrain "+ folder + " Class: ", len(os.listdir(train_folder + '/' + folder)))

print("\n\n", train_c)

print("\n\n\t\tTesting Set")
print("\t  ========================\n")
test_c = []
for folder in os.listdir(test_folder):
    test_c.append(folder)
    print("\nTest "+ folder + " Class: ", len(os.listdir(test_folder + '/' + folder)))

print("\n\n", test_c)

print("\n\n\t\tValidation Set")
print("\t  ========================\n")
val_c = []
for folder in os.listdir(valid_folder):
    val_c.append(folder)
    print("\nValidation "+ folder + " Class: ", len(os.listdir(valid_folder + '/' + folder)))

print("\n\n", val_c, "\n")

input_shape = (224,224,3)
num_class = 4

train_datagen = ImageDataGenerator(
    dtype='float32',
    preprocessing_function=preprocess_input,
    rotation_range=10,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=False
)

val_datagen = ImageDataGenerator(
    dtype='float32',
    preprocessing_function=preprocess_input,
)

test_datagen = ImageDataGenerator(
    dtype='float32',
    preprocessing_function=preprocess_input,
)

train_generator = train_datagen.flow_from_directory(
    train_folder,
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
)


test_generator = test_datagen.flow_from_directory(
    test_folder,
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
    shuffle = False,
)

validation_generator = val_datagen.flow_from_directory(
    valid_folder,
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
)

"""### VGG16 Model"""

modelVGG16 = VGG16(weights = 'imagenet',
                     include_top = False,
                     input_shape = input_shape)

for layer in modelVGG16.layers:
    layer.trainable = False

model_16 = Sequential([
    modelVGG16,
    BatchNormalization(),
    MaxPooling2D(pool_size = (2,2)),
    Dropout(.3),
    Flatten(),
    Dense(1024, activation = 'relu'),
    Dropout(.5),
    Dense(512, activation = 'relu'),
    Dropout(.5),
    Dense(256, activation = 'relu'),
    Dropout(.5),
    Dense(num_class, activation = 'softmax')
])


print(model_16.summary())

opt = tf.keras.optimizers.Adam(learning_rate = 0.001)
opt1 = tf.keras.optimizers.RMSprop(learning_rate = 0.001)

model_16.compile(loss = 'categorical_crossentropy',
             optimizer = opt,
             metrics = ['accuracy'])

checkpoint = ModelCheckpoint(filepath = 'C_CT_S.h5',
                            monitor = 'val_accuracy',
                            mode = 'max',
                            save_best_only = True,
                            verbose = 1)

earlystop = EarlyStopping(monitor = 'val_accuracy',
                         min_delta = .5,
                         patience = 5,
                         restore_best_weights = True)

reduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy',
                             factor = 0.1,
                             patience = 3,
                             verbose = 1,
                             min_delta = 0.8)

callbacks = [earlystop, reduce_lr]

epochs = 10

history = model_16.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=epochs,
    verbose=1
)
#test = test_generator
#score = model.evaluate(test, verbose = 1)
#print("Test loss:", score[0])
#print("Test accuracy:", score[1])

test = test_generator
score = model_16.evaluate(test, verbose = 1)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

num_test_samples = len(test_generator)
num_classes = len(test_generator.class_indices)

predicted_probabilities = model_16.predict(test_generator, steps=num_test_samples)
predicted_labels = np.argmax(predicted_probabilities, axis=1)

true_labels = test_generator.classes

report = classification_report(true_labels, predicted_labels)

print(report)

plt.figure(figsize=(15, 7))
plt.plot(range(epochs), history.history['accuracy'])
plt.plot(range(epochs), history.history['val_accuracy'])
plt.legend(['training_acc', 'validation_acc'])
plt.title('Accuracy')

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(loss))
fig = plt.figure(figsize=(14,7))
plt.plot(epochs,loss,'r',label="Training loss")
plt.plot(epochs,val_loss,'b',label="Validation loss")
plt.legend(loc='upper left')
plt.show()

num_test_samples = len(test_generator)
num_classes = len(test_generator.class_indices)

predicted_probabilities = model_16.predict(test_generator, steps=num_test_samples)
predicted_labels = np.argmax(predicted_probabilities, axis=1)

true_labels = test_generator.classes

report = classification_report(true_labels, predicted_labels)

print(report)

cm = confusion_matrix(true_labels, predicted_labels)
print("\n\nConfusion Matrix:\n", cm)

plt.figure(figsize=(10,4))
sns.heatmap(cm, annot=True, fmt='g', cmap='Greens', xticklabels = test_c, yticklabels = test_c)
plt.xlabel('\n\nPredicted Label\n')
plt.ylabel('\nTrue Label\n')
plt.title('Confusion Matrix\n\n')
plt.show()

#Plotting the loss charts

history_dict = history.history

loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(loss_values) + 1)

line1 = plt.plot(epochs, val_loss_values, label = 'Validation/Test Loss')
line2 = plt.plot(epochs, loss_values, label = 'Training Loss')

plt.setp(line1, linewidth = 1.8, marker = 'o', markersize = 6.0)
plt.setp(line2, linewidth = 1.5, marker = '*', markersize = 8.0)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()

plt.show()

#Plotting the accuracy charts

history_dict = history.history

acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']
epochs = range(1, len(acc_values) + 1)

line1 = plt.plot(epochs, val_acc_values, label = 'Validation/Test Accuracy')
line2 = plt.plot(epochs, acc_values, label = 'Training Accuracy')

plt.setp(line1, linewidth = 1.8, marker = 'o', markersize = 6.5)
plt.setp(line2, linewidth = 1.8, marker = '*', markersize = 8.0)
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.grid(True)
plt.legend()
plt.show()

"""### ResNet50 Model"""

ResNet50_model = ResNet50(weights = 'imagenet',
                     include_top = False,
                     input_shape = input_shape)

for layer in ResNet50_model.layers:
    layer.trainable = False

num_classes = len(test_generator.class_indices)

# Define a Checkpoint
checkpoint = ModelCheckpoint(filepath='C_CT_S.h5',
                             monitor='val_accuracy',
                             mode='max',
                             save_best_only=True,
                             verbose=1)

# Define Early Stopping
earlystop = EarlyStopping(monitor='val_accuracy',
                         min_delta=0.001,  # Adjust the min_delta value as needed
                         patience=15,
                         restore_best_weights=True)

# Define LR Reducing Rate
reduce_lr = ReduceLROnPlateau(monitor='val_loss',  # You can use 'val_loss' for learning rate reduction
                              factor=0.1,
                              patience=10,
                              verbose=1,
                              min_delta=0.0001,
                              min_lr=0.0001)

# Put the callbacks in a callback list
callbacks = [reduce_lr]

model_50 = Sequential([
    ResNet50_model,
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.3),
    Flatten(),
    Dense(1024, activation='relu'),
    Dropout(0.3),
    Dense(512, activation='relu'),
    Dropout(0.3),
    Dense(256, activation='relu'),
    Dropout(0.3),
    Dense(num_classes, activation='softmax')
])

model_50.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 75

history = model_50.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=epochs,
    callbacks=callbacks,  # Include the defined callbacks
    verbose=1
)

test = test_generator
score = model_50.evaluate(test, verbose = 1)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

num_test_samples = len(test_generator)
num_classes = len(test_generator.class_indices)

predicted_probabilities = model_50.predict(test_generator, steps=num_test_samples)
predicted_labels = np.argmax(predicted_probabilities, axis=1)

true_labels = test_generator.classes

report = classification_report(true_labels, predicted_labels)

print(report)

cm = confusion_matrix(true_labels, predicted_labels)
print("\n\nConfusion Matrix:\n", cm)

plt.figure(figsize=(10,4))
sns.heatmap(cm, annot=True, fmt='g', cmap='Greens', xticklabels = test_c, yticklabels = test_c)
plt.xlabel('\n\nPredicted Label\n')
plt.ylabel('\nTrue Label\n')
plt.title('Confusion Matrix\n\n')
plt.show()

#Plotting the loss charts

history_dict = history.history

loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(loss_values) + 1)

line1 = plt.plot(epochs, val_loss_values, label = 'Validation/Test Loss')
line2 = plt.plot(epochs, loss_values, label = 'Training Loss')

plt.setp(line1, linewidth = 1.8, marker = 'o', markersize = 6.0)
plt.setp(line2, linewidth = 1.5, marker = '*', markersize = 8.0)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()

plt.show()

#Plotting the accuracy charts

history_dict = history.history

acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']
epochs = range(1, len(acc_values) + 1)

line1 = plt.plot(epochs, val_acc_values, label = 'Validation/Test Accuracy')
line2 = plt.plot(epochs, acc_values, label = 'Training Accuracy')

plt.setp(line1, linewidth = 1.8, marker = 'o', markersize = 6.5)
plt.setp(line2, linewidth = 1.8, marker = '*', markersize = 8.0)
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.grid(True)
plt.legend()
plt.show()

"""### Интерфейс"""

import streamlit as st
import pickle
from PIL import Image
import io



with open('model.pkl', 'rb') as f:
   model = pickle.load(f)

def load_image():
    uploaded_file = st.file_uploader(label='Выберите изображение для распознавания')
    if uploaded_file is not None:
        image_data = uploaded_file.getvalue()
        st.image(image_data)
        return Image.open(io.BytesIO(image_data))
    else:
        return None

st.title('F')
img = load_image()

if(st.button('Submit')):
    st.success(img)
    #st.write(result)